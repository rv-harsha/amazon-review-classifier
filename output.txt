For key AdaBoost
Fitting 5 folds for each of 16 candidates, totaling 80 fits
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=30; total time=  21.4s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=30; total time=  21.1s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=30; total time=  21.3s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=30; total time=  22.6s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=30; total time=  22.0s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=50; total time=  35.6s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=50; total time=  35.8s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=50; total time=  35.2s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=50; total time=  35.3s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=50; total time=  36.7s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=70; total time=  50.7s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=70; total time=  51.0s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=70; total time=  50.4s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=70; total time=  49.7s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=70; total time=  50.4s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=80; total time=  57.6s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=80; total time=  57.5s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=80; total time=  57.5s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=80; total time=  56.9s
[CV] END algorithm=SAMME, learning_rate=0.001, n_estimators=80; total time=  57.4s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=30; total time=  21.5s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=30; total time=  21.7s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=30; total time=  21.7s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=30; total time=  22.0s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=30; total time=  22.3s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=  37.6s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=  37.4s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=  36.6s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=  36.6s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=  35.8s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=70; total time=  50.1s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=70; total time=  50.2s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=70; total time=  51.4s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=70; total time=  50.0s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=70; total time=  50.3s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=80; total time=  57.9s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=80; total time=  57.8s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=80; total time=  57.3s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=80; total time=  56.7s
[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=80; total time=  56.5s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=30; total time=  21.7s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=30; total time=  21.8s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=30; total time=  21.8s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=30; total time=  21.9s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=30; total time=  21.8s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=50; total time=  35.9s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=50; total time=  37.5s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=50; total time=  36.8s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=50; total time=  36.0s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=50; total time=  36.1s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=70; total time=  50.1s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=70; total time=  50.7s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=70; total time=  50.0s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=70; total time=  53.3s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=70; total time=  52.4s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=80; total time=  58.2s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=80; total time=  58.6s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=80; total time=  58.4s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=80; total time=  58.4s
[CV] END algorithm=SAMME, learning_rate=0.8, n_estimators=80; total time=  59.8s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=30; total time=  22.6s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=30; total time=  21.8s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=30; total time=  21.5s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=30; total time=  21.5s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=30; total time=  22.0s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=50; total time=  36.0s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=50; total time=  37.1s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=50; total time=  38.1s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=50; total time=  37.6s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=50; total time=  37.9s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=70; total time=  50.9s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=70; total time=  51.4s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=70; total time=  50.0s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=70; total time=  49.6s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=70; total time=  49.6s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=80; total time=  56.0s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=80; total time=  55.9s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=80; total time=  56.2s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=80; total time=  56.0s
[CV] END algorithm=SAMME, learning_rate=1.0, n_estimators=80; total time=  55.8s
AdaBoostClassifier(algorithm='SAMME', n_estimators=80, random_state=42)
{'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 80}
0.7341394271528053
Best ROC_AUC Score on Validation Set: 0.740801127563661
Grid search completed for AdaBoost


Fitting grid search for RandomForest
Fitting 5 folds for each of 27 candidates, totalling 135 fits
[CV] END .max_depth=5, min_samples_split=2, n_estimators=100; total time=   4.9s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=100; total time=   3.2s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=100; total time=   2.8s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=100; total time=   3.0s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=100; total time=   3.2s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=200; total time=   5.6s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=200; total time=   4.4s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=200; total time=   6.3s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=200; total time=   6.2s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=200; total time=   5.4s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=300; total time=   8.4s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=300; total time=   7.9s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=300; total time=   8.3s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=300; total time=   7.8s
[CV] END .max_depth=5, min_samples_split=2, n_estimators=300; total time=   7.3s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=100; total time=   2.2s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=100; total time=   2.4s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=100; total time=   3.1s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=100; total time=   2.9s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=100; total time=   2.9s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=200; total time=   4.4s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=200; total time=   5.2s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=200; total time=   4.3s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=200; total time=   5.0s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=200; total time=   4.3s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=300; total time=   7.7s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=300; total time=   7.7s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=300; total time=   7.3s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=300; total time=   7.4s
[CV] END .max_depth=5, min_samples_split=5, n_estimators=300; total time=   7.7s
[CV] END max_depth=5, min_samples_split=10, n_estimators=100; total time=   2.9s
[CV] END max_depth=5, min_samples_split=10, n_estimators=100; total time=   2.9s
[CV] END max_depth=5, min_samples_split=10, n_estimators=100; total time=   2.8s
[CV] END max_depth=5, min_samples_split=10, n_estimators=100; total time=   2.8s
[CV] END max_depth=5, min_samples_split=10, n_estimators=100; total time=   2.8s
[CV] END max_depth=5, min_samples_split=10, n_estimators=200; total time=   5.2s
[CV] END max_depth=5, min_samples_split=10, n_estimators=200; total time=   5.2s
[CV] END max_depth=5, min_samples_split=10, n_estimators=200; total time=   5.1s
[CV] END max_depth=5, min_samples_split=10, n_estimators=200; total time=   5.1s
[CV] END max_depth=5, min_samples_split=10, n_estimators=200; total time=   5.2s
[CV] END max_depth=5, min_samples_split=10, n_estimators=300; total time=   7.7s
[CV] END max_depth=5, min_samples_split=10, n_estimators=300; total time=   7.3s
[CV] END max_depth=5, min_samples_split=10, n_estimators=300; total time=   7.3s
[CV] END max_depth=5, min_samples_split=10, n_estimators=300; total time=   5.9s
[CV] END max_depth=5, min_samples_split=10, n_estimators=300; total time=   7.2s
[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s
[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.7s
[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s
[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s
[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s
[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   6.9s
[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   7.2s
[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   7.0s
[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   6.5s
[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   6.4s
[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   9.9s
[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=  10.2s
[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=  10.2s
[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=  10.0s
[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=   9.9s
[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.6s
[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.6s
[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.6s
[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.5s
[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.5s
[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   6.6s
[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   7.5s
[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   6.5s
[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   7.2s
[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   6.9s
[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   9.8s
[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=  10.4s
[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=  10.2s
[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   9.9s
[CV] END max_depth=10, min_samples_split=5, n_estimators=300; total time=   9.6s
[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.9s
[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   4.1s
[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.7s
[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.9s
[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.7s
[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   7.0s
[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   6.9s
[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   7.4s
[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   7.0s
[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   7.1s
[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=  10.4s
[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   9.9s
[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   9.7s
[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   9.5s
[CV] END max_depth=10, min_samples_split=10, n_estimators=300; total time=   9.7s
[CV] END max_depth=18, min_samples_split=2, n_estimators=100; total time=   4.8s
[CV] END max_depth=18, min_samples_split=2, n_estimators=100; total time=   5.3s
[CV] END max_depth=18, min_samples_split=2, n_estimators=100; total time=   5.5s
[CV] END max_depth=18, min_samples_split=2, n_estimators=100; total time=   5.3s
[CV] END max_depth=18, min_samples_split=2, n_estimators=100; total time=   5.6s
[CV] END max_depth=18, min_samples_split=2, n_estimators=200; total time=  10.7s
[CV] END max_depth=18, min_samples_split=2, n_estimators=200; total time=   9.6s
[CV] END max_depth=18, min_samples_split=2, n_estimators=200; total time=   9.0s
[CV] END max_depth=18, min_samples_split=2, n_estimators=200; total time=   9.5s
[CV] END max_depth=18, min_samples_split=2, n_estimators=200; total time=   9.4s
[CV] END max_depth=18, min_samples_split=2, n_estimators=300; total time=  14.0s
[CV] END max_depth=18, min_samples_split=2, n_estimators=300; total time=  15.8s
[CV] END max_depth=18, min_samples_split=2, n_estimators=300; total time=  15.7s
[CV] END max_depth=18, min_samples_split=2, n_estimators=300; total time=  14.9s
[CV] END max_depth=18, min_samples_split=2, n_estimators=300; total time=  15.0s
[CV] END max_depth=18, min_samples_split=5, n_estimators=100; total time=   4.8s
[CV] END max_depth=18, min_samples_split=5, n_estimators=100; total time=   4.9s
[CV] END max_depth=18, min_samples_split=5, n_estimators=100; total time=   5.0s
[CV] END max_depth=18, min_samples_split=5, n_estimators=100; total time=   4.9s
[CV] END max_depth=18, min_samples_split=5, n_estimators=100; total time=   4.9s
[CV] END max_depth=18, min_samples_split=5, n_estimators=200; total time=   9.7s
[CV] END max_depth=18, min_samples_split=5, n_estimators=200; total time=   8.9s
[CV] END max_depth=18, min_samples_split=5, n_estimators=200; total time=   8.8s
[CV] END max_depth=18, min_samples_split=5, n_estimators=200; total time=   9.0s
[CV] END max_depth=18, min_samples_split=5, n_estimators=200; total time=   8.8s
[CV] END max_depth=18, min_samples_split=5, n_estimators=300; total time=  13.7s
[CV] END max_depth=18, min_samples_split=5, n_estimators=300; total time=  14.0s
[CV] END max_depth=18, min_samples_split=5, n_estimators=300; total time=  13.7s
[CV] END max_depth=18, min_samples_split=5, n_estimators=300; total time=  14.3s
[CV] END max_depth=18, min_samples_split=5, n_estimators=300; total time=  13.5s
[CV] END max_depth=18, min_samples_split=10, n_estimators=100; total time=   4.6s
[CV] END max_depth=18, min_samples_split=10, n_estimators=100; total time=   4.6s
[CV] END max_depth=18, min_samples_split=10, n_estimators=100; total time=   5.1s
[CV] END max_depth=18, min_samples_split=10, n_estimators=100; total time=   5.4s
[CV] END max_depth=18, min_samples_split=10, n_estimators=100; total time=   4.8s
[CV] END max_depth=18, min_samples_split=10, n_estimators=200; total time=   9.6s
[CV] END max_depth=18, min_samples_split=10, n_estimators=200; total time=   9.8s
[CV] END max_depth=18, min_samples_split=10, n_estimators=200; total time=   9.3s
[CV] END max_depth=18, min_samples_split=10, n_estimators=200; total time=  10.1s
[CV] END max_depth=18, min_samples_split=10, n_estimators=200; total time=   9.4s
[CV] END max_depth=18, min_samples_split=10, n_estimators=300; total time=  13.9s
[CV] END max_depth=18, min_samples_split=10, n_estimators=300; total time=  13.9s
[CV] END max_depth=18, min_samples_split=10, n_estimators=300; total time=  14.0s
[CV] END max_depth=18, min_samples_split=10, n_estimators=300; total time=  14.0s
[CV] END max_depth=18, min_samples_split=10, n_estimators=300; total time=  13.2s
RandomForestClassifier(max_depth=18, min_samples_split=10, n_estimators=300,
                       n_jobs=-1)
{'max_depth': 18, 'min_samples_split': 10, 'n_estimators': 300}
0.7688816957004523
Best ROC_AUC Score on Validation Set: 0.7763698555622247
Grid search completed for RandomForest


Fitting grid search for SGD
For key SGD
Fitting 5 folds for each of 48 candidates, totalling 240 fits
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   1.3s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   1.0s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   1.1s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   1.1s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l2; total time=   1.1s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.6s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.6s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.7s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.6s
[CV] END ...............alpha=0.0001, loss=hinge, penalty=l1; total time=   1.6s
[CV] END .............alpha=0.0001, loss=hinge, penalty=none; total time=   1.4s
[CV] END .............alpha=0.0001, loss=hinge, penalty=none; total time=   1.4s
[CV] END .............alpha=0.0001, loss=hinge, penalty=none; total time=   1.4s
[CV] END .............alpha=0.0001, loss=hinge, penalty=none; total time=   1.3s
[CV] END .............alpha=0.0001, loss=hinge, penalty=none; total time=   1.4s
[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   1.2s
[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   1.1s
[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   1.1s
[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   1.1s
[CV] END .................alpha=0.0001, loss=log, penalty=l2; total time=   1.2s
[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   1.6s
[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   1.6s
[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   1.6s
[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   1.6s
[CV] END .................alpha=0.0001, loss=log, penalty=l1; total time=   1.6s
[CV] END ...............alpha=0.0001, loss=log, penalty=none; total time=   1.2s
[CV] END ...............alpha=0.0001, loss=log, penalty=none; total time=   1.1s
[CV] END ...............alpha=0.0001, loss=log, penalty=none; total time=   1.2s
[CV] END ...............alpha=0.0001, loss=log, penalty=none; total time=   1.2s
[CV] END ...............alpha=0.0001, loss=log, penalty=none; total time=   1.2s
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .......alpha=0.0001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .....alpha=0.0001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .....alpha=0.0001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .....alpha=0.0001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .....alpha=0.0001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .....alpha=0.0001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   2.7s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   2.3s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   2.2s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   2.5s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l2; total time=   2.4s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l1; total time=   3.9s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l1; total time=   3.2s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l1; total time=   3.4s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l1; total time=   3.8s
[CV] END ......alpha=0.0001, loss=modified_huber, penalty=l1; total time=   3.7s
[CV] END ....alpha=0.0001, loss=modified_huber, penalty=none; total time=   2.4s
[CV] END ....alpha=0.0001, loss=modified_huber, penalty=none; total time=   2.1s
[CV] END ....alpha=0.0001, loss=modified_huber, penalty=none; total time=   2.0s
[CV] END ....alpha=0.0001, loss=modified_huber, penalty=none; total time=   2.3s
[CV] END ....alpha=0.0001, loss=modified_huber, penalty=none; total time=   2.3s
[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.7s
[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.7s
[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.7s
[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.7s
[CV] END ................alpha=0.001, loss=hinge, penalty=l2; total time=   0.7s
[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.0s
[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.1s
[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.1s
[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.1s
[CV] END ................alpha=0.001, loss=hinge, penalty=l1; total time=   1.1s
[CV] END ..............alpha=0.001, loss=hinge, penalty=none; total time=   1.0s
[CV] END ..............alpha=0.001, loss=hinge, penalty=none; total time=   1.1s
[CV] END ..............alpha=0.001, loss=hinge, penalty=none; total time=   1.1s
[CV] END ..............alpha=0.001, loss=hinge, penalty=none; total time=   1.1s
[CV] END ..............alpha=0.001, loss=hinge, penalty=none; total time=   1.1s
[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.8s
[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.8s
[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.8s
[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.8s
[CV] END ..................alpha=0.001, loss=log, penalty=l2; total time=   0.8s
[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.2s
[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.2s
[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.2s
[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.2s
[CV] END ..................alpha=0.001, loss=log, penalty=l1; total time=   1.2s
[CV] END ................alpha=0.001, loss=log, penalty=none; total time=   1.0s
[CV] END ................alpha=0.001, loss=log, penalty=none; total time=   1.1s
[CV] END ................alpha=0.001, loss=log, penalty=none; total time=   1.1s
[CV] END ................alpha=0.001, loss=log, penalty=none; total time=   1.0s
[CV] END ................alpha=0.001, loss=log, penalty=none; total time=   1.0s
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l1; total time= 1.8min
[CV] END ........alpha=0.001, loss=squared_hinge, penalty=l1; total time= 1.8min
[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END ......alpha=0.001, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l2; total time=   1.0s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l2; total time=   1.1s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l2; total time=   1.1s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l2; total time=   1.0s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l2; total time=   1.0s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l1; total time=   1.9s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l1; total time=   2.2s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l1; total time=   1.7s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l1; total time=   1.7s
[CV] END .......alpha=0.001, loss=modified_huber, penalty=l1; total time=   1.9s
[CV] END .....alpha=0.001, loss=modified_huber, penalty=none; total time=   3.3s
[CV] END .....alpha=0.001, loss=modified_huber, penalty=none; total time=   3.4s
[CV] END .....alpha=0.001, loss=modified_huber, penalty=none; total time=   3.0s
[CV] END .....alpha=0.001, loss=modified_huber, penalty=none; total time=   3.1s
[CV] END .....alpha=0.001, loss=modified_huber, penalty=none; total time=   3.8s
[CV] END .................alpha=0.01, loss=hinge, penalty=l2; total time=   0.6s
[CV] END .................alpha=0.01, loss=hinge, penalty=l2; total time=   0.6s
[CV] END .................alpha=0.01, loss=hinge, penalty=l2; total time=   0.6s
[CV] END .................alpha=0.01, loss=hinge, penalty=l2; total time=   0.6s
[CV] END .................alpha=0.01, loss=hinge, penalty=l2; total time=   0.6s
[CV] END .................alpha=0.01, loss=hinge, penalty=l1; total time=   0.9s
[CV] END .................alpha=0.01, loss=hinge, penalty=l1; total time=   0.9s
[CV] END .................alpha=0.01, loss=hinge, penalty=l1; total time=   0.9s
[CV] END .................alpha=0.01, loss=hinge, penalty=l1; total time=   1.0s
[CV] END .................alpha=0.01, loss=hinge, penalty=l1; total time=   0.9s
[CV] END ...............alpha=0.01, loss=hinge, penalty=none; total time=   0.8s
[CV] END ...............alpha=0.01, loss=hinge, penalty=none; total time=   0.8s
[CV] END ...............alpha=0.01, loss=hinge, penalty=none; total time=   0.8s
[CV] END ...............alpha=0.01, loss=hinge, penalty=none; total time=   0.7s
[CV] END ...............alpha=0.01, loss=hinge, penalty=none; total time=   0.7s
[CV] END ...................alpha=0.01, loss=log, penalty=l2; total time=   0.7s
[CV] END ...................alpha=0.01, loss=log, penalty=l2; total time=   0.7s
[CV] END ...................alpha=0.01, loss=log, penalty=l2; total time=   0.7s
[CV] END ...................alpha=0.01, loss=log, penalty=l2; total time=   0.7s
[CV] END ...................alpha=0.01, loss=log, penalty=l2; total time=   0.8s
[CV] END ...................alpha=0.01, loss=log, penalty=l1; total time=   1.1s
[CV] END ...................alpha=0.01, loss=log, penalty=l1; total time=   1.1s
[CV] END ...................alpha=0.01, loss=log, penalty=l1; total time=   1.0s
[CV] END ...................alpha=0.01, loss=log, penalty=l1; total time=   1.1s
[CV] END ...................alpha=0.01, loss=log, penalty=l1; total time=   1.1s
[CV] END .................alpha=0.01, loss=log, penalty=none; total time=   1.0s
[CV] END .................alpha=0.01, loss=log, penalty=none; total time=   1.0s
[CV] END .................alpha=0.01, loss=log, penalty=none; total time=   1.0s
[CV] END .................alpha=0.01, loss=log, penalty=none; total time=   1.0s
[CV] END .................alpha=0.01, loss=log, penalty=none; total time=   1.0s
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l2; total time=  11.3s
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l2; total time= 1.1min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l1; total time= 1.8min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l1; total time= 1.8min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .........alpha=0.01, loss=squared_hinge, penalty=l1; total time= 1.9min
[CV] END .......alpha=0.01, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .......alpha=0.01, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .......alpha=0.01, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .......alpha=0.01, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END .......alpha=0.01, loss=squared_hinge, penalty=none; total time= 1.1min
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l2; total time=   0.9s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END ........alpha=0.01, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END ......alpha=0.01, loss=modified_huber, penalty=none; total time=   2.8s
[CV] END ......alpha=0.01, loss=modified_huber, penalty=none; total time=   2.5s
[CV] END ......alpha=0.01, loss=modified_huber, penalty=none; total time=   2.6s
[CV] END ......alpha=0.01, loss=modified_huber, penalty=none; total time=   2.6s
[CV] END ......alpha=0.01, loss=modified_huber, penalty=none; total time=   3.5s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l2; total time=   0.5s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l2; total time=   0.5s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l2; total time=   0.5s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l2; total time=   0.5s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l2; total time=   0.5s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l1; total time=   0.8s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l1; total time=   0.8s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l1; total time=   0.8s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l1; total time=   0.8s
[CV] END ..................alpha=0.1, loss=hinge, penalty=l1; total time=   0.8s
[CV] END ................alpha=0.1, loss=hinge, penalty=none; total time=   0.6s
[CV] END ................alpha=0.1, loss=hinge, penalty=none; total time=   0.6s
[CV] END ................alpha=0.1, loss=hinge, penalty=none; total time=   0.6s
[CV] END ................alpha=0.1, loss=hinge, penalty=none; total time=   0.6s
[CV] END ................alpha=0.1, loss=hinge, penalty=none; total time=   0.6s
[CV] END ....................alpha=0.1, loss=log, penalty=l2; total time=   0.7s
[CV] END ....................alpha=0.1, loss=log, penalty=l2; total time=   0.7s
[CV] END ....................alpha=0.1, loss=log, penalty=l2; total time=   0.7s
[CV] END ....................alpha=0.1, loss=log, penalty=l2; total time=   0.7s
[CV] END ....................alpha=0.1, loss=log, penalty=l2; total time=   0.7s
[CV] END ....................alpha=0.1, loss=log, penalty=l1; total time=   1.0s
[CV] END ....................alpha=0.1, loss=log, penalty=l1; total time=   1.0s
[CV] END ....................alpha=0.1, loss=log, penalty=l1; total time=   1.0s
[CV] END ....................alpha=0.1, loss=log, penalty=l1; total time=   1.0s
[CV] END ....................alpha=0.1, loss=log, penalty=l1; total time=   1.0s
[CV] END ..................alpha=0.1, loss=log, penalty=none; total time=   0.8s
[CV] END ..................alpha=0.1, loss=log, penalty=none; total time=   0.8s
[CV] END ..................alpha=0.1, loss=log, penalty=none; total time=   0.8s
[CV] END ..................alpha=0.1, loss=log, penalty=none; total time=   0.8s
[CV] END ..................alpha=0.1, loss=log, penalty=none; total time=   0.8s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l2; total time=   0.8s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l2; total time=   0.8s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l2; total time=   0.8s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l2; total time=   0.8s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l2; total time=   0.8s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l1; total time=   1.2s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l1; total time=   1.2s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l1; total time=   3.1s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l1; total time=  39.8s
[CV] END ..........alpha=0.1, loss=squared_hinge, penalty=l1; total time=   1.2s
[CV] END ........alpha=0.1, loss=squared_hinge, penalty=none; total time=   1.5s
[CV] END ........alpha=0.1, loss=squared_hinge, penalty=none; total time=   1.1s
[CV] END ........alpha=0.1, loss=squared_hinge, penalty=none; total time=   1.7s
[CV] END ........alpha=0.1, loss=squared_hinge, penalty=none; total time=   2.3s
[CV] END ........alpha=0.1, loss=squared_hinge, penalty=none; total time=   1.1s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l2; total time=   0.8s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END .........alpha=0.1, loss=modified_huber, penalty=l1; total time=   1.2s
[CV] END .......alpha=0.1, loss=modified_huber, penalty=none; total time=   1.2s
[CV] END .......alpha=0.1, loss=modified_huber, penalty=none; total time=   1.1s
[CV] END .......alpha=0.1, loss=modified_huber, penalty=none; total time=   1.1s
[CV] END .......alpha=0.1, loss=modified_huber, penalty=none; total time=   1.1s
[CV] END .......alpha=0.1, loss=modified_huber, penalty=none; total time=   1.1s
SGDClassifier(loss='log')
{'alpha': 0.0001, 'loss': 'log', 'penalty': 'l2'}
0.7880527549093188
Best ROC_AUC Score on Validation Set: 0.7943409508642552
Grid search completed for SGD




base) harsha@MacBook-Pro code % python semi-supervised-learning.py
[nltk_data] Downloading package stopwords to
[nltk_data]     /Users/harsha/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Supervised SGDClassifier on 100% of the data:
Number of training samples: 57482
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.831
----------

Supervised SGDClassifier on 2.0% of the training data:
Number of training samples: 1149
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.766
----------

SelfTrainingClassifier on 2.0% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 56333
End of iteration 1, added 50084 new labels.
End of iteration 2, added 977 new labels.
End of iteration 3, added 567 new labels.
End of iteration 4, added 399 new labels.
End of iteration 5, added 31 new labels.
End of iteration 6, added 1334 new labels.
End of iteration 7, added 123 new labels.
End of iteration 8, added 177 new labels.
Micro-averaged F1 score on test set: 0.771
----------

Supervised SGDClassifier on 4.416666666666667% of the training data:
Number of training samples: 2538
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.745
----------

SelfTrainingClassifier on 4.416666666666667% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 54944
End of iteration 1, added 46055 new labels.
End of iteration 2, added 2318 new labels.
End of iteration 3, added 2259 new labels.
End of iteration 4, added 1103 new labels.
End of iteration 5, added 296 new labels.
End of iteration 6, added 171 new labels.
End of iteration 7, added 27 new labels.
End of iteration 8, added 19 new labels.
End of iteration 9, added 1 new labels.
Micro-averaged F1 score on test set: 0.790
----------

Supervised SGDClassifier on 6.833333333333333% of the training data:
Number of training samples: 3927
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.788
----------

SelfTrainingClassifier on 6.833333333333333% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 53555
End of iteration 1, added 44154 new labels.
End of iteration 2, added 4281 new labels.
End of iteration 3, added 1284 new labels.
End of iteration 4, added 114 new labels.
End of iteration 5, added 353 new labels.
End of iteration 6, added 874 new labels.
End of iteration 7, added 5 new labels.
End of iteration 8, added 205 new labels.
End of iteration 9, added 60 new labels.
End of iteration 10, added 361 new labels.
Micro-averaged F1 score on test set: 0.811
----------

Supervised SGDClassifier on 9.25% of the training data:
Number of training samples: 5317
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.789
----------

SelfTrainingClassifier on 9.25% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 52165
End of iteration 1, added 40691 new labels.
End of iteration 2, added 5862 new labels.
End of iteration 3, added 2051 new labels.
End of iteration 4, added 691 new labels.
End of iteration 5, added 1393 new labels.
End of iteration 6, added 3 new labels.
End of iteration 7, added 122 new labels.
End of iteration 8, added 2 new labels.
Micro-averaged F1 score on test set: 0.823
----------

Supervised SGDClassifier on 11.666666666666666% of the training data:
Number of training samples: 6706
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.786
----------

SelfTrainingClassifier on 11.666666666666666% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 50776
End of iteration 1, added 39387 new labels.
End of iteration 2, added 7369 new labels.
End of iteration 3, added 1093 new labels.
End of iteration 4, added 1067 new labels.
End of iteration 5, added 222 new labels.
End of iteration 6, added 77 new labels.
End of iteration 7, added 12 new labels.
End of iteration 8, added 14 new labels.
End of iteration 9, added 1 new labels.
End of iteration 10, added 14 new labels.
Micro-averaged F1 score on test set: 0.823
----------

Supervised SGDClassifier on 14.083333333333334% of the training data:
Number of training samples: 8095
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.810
----------

SelfTrainingClassifier on 14.083333333333334% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 49387
End of iteration 1, added 41559 new labels.
End of iteration 2, added 4999 new labels.
End of iteration 3, added 994 new labels.
End of iteration 4, added 337 new labels.
End of iteration 5, added 455 new labels.
End of iteration 6, added 142 new labels.
End of iteration 7, added 574 new labels.
Micro-averaged F1 score on test set: 0.823
----------

Supervised SGDClassifier on 16.499999999999996% of the training data:
Number of training samples: 9484
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.804
----------

SelfTrainingClassifier on 16.499999999999996% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 47998
End of iteration 1, added 37259 new labels.
End of iteration 2, added 5853 new labels.
End of iteration 3, added 1692 new labels.
End of iteration 4, added 90 new labels.
End of iteration 5, added 1218 new labels.
End of iteration 6, added 226 new labels.
End of iteration 7, added 406 new labels.
End of iteration 8, added 58 new labels.
End of iteration 9, added 116 new labels.
End of iteration 10, added 1 new labels.
Micro-averaged F1 score on test set: 0.826
----------

Supervised SGDClassifier on 18.916666666666664% of the training data:
Number of training samples: 10873
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.825
----------

SelfTrainingClassifier on 18.916666666666664% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 46609
End of iteration 1, added 35319 new labels.
End of iteration 2, added 5647 new labels.
End of iteration 3, added 1754 new labels.
End of iteration 4, added 1112 new labels.
End of iteration 5, added 880 new labels.
End of iteration 6, added 921 new labels.
End of iteration 7, added 5 new labels.
End of iteration 8, added 330 new labels.
End of iteration 9, added 1 new labels.
Micro-averaged F1 score on test set: 0.825
----------

Supervised SGDClassifier on 21.333333333333332% of the training data:
Number of training samples: 12262
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.823
----------

SelfTrainingClassifier on 21.333333333333332% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 45220
End of iteration 1, added 36370 new labels.
End of iteration 2, added 2809 new labels.
End of iteration 3, added 2537 new labels.
End of iteration 4, added 1717 new labels.
End of iteration 5, added 621 new labels.
End of iteration 6, added 7 new labels.
End of iteration 7, added 38 new labels.
End of iteration 8, added 28 new labels.
End of iteration 9, added 2 new labels.
End of iteration 10, added 17 new labels.
Micro-averaged F1 score on test set: 0.827
----------

Supervised SGDClassifier on 23.75% of the training data:
Number of training samples: 13651
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.827
----------

SelfTrainingClassifier on 23.75% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 43831
End of iteration 1, added 32491 new labels.
End of iteration 2, added 6705 new labels.
End of iteration 3, added 2198 new labels.
End of iteration 4, added 1149 new labels.
End of iteration 5, added 29 new labels.
End of iteration 6, added 88 new labels.
End of iteration 7, added 524 new labels.
End of iteration 8, added 85 new labels.
Micro-averaged F1 score on test set: 0.824
----------

Supervised SGDClassifier on 26.166666666666664% of the training data:
Number of training samples: 15041
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.826
----------

SelfTrainingClassifier on 26.166666666666664% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 42441
End of iteration 1, added 30643 new labels.
End of iteration 2, added 4997 new labels.
End of iteration 3, added 2840 new labels.
End of iteration 4, added 2004 new labels.
End of iteration 5, added 14 new labels.
End of iteration 6, added 286 new labels.
End of iteration 7, added 2 new labels.
Micro-averaged F1 score on test set: 0.829
----------

Supervised SGDClassifier on 28.583333333333332% of the training data:
Number of training samples: 16430
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.828
----------

SelfTrainingClassifier on 28.583333333333332% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 41052
End of iteration 1, added 25928 new labels.
End of iteration 2, added 8719 new labels.
End of iteration 3, added 1288 new labels.
End of iteration 4, added 1316 new labels.
End of iteration 5, added 165 new labels.
End of iteration 6, added 126 new labels.
End of iteration 7, added 20 new labels.
End of iteration 8, added 31 new labels.
End of iteration 9, added 5 new labels.
End of iteration 10, added 63 new labels.
Micro-averaged F1 score on test set: 0.830
----------

Supervised SGDClassifier on 31.0% of the training data:
Number of training samples: 17819
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.829
----------

SelfTrainingClassifier on 31.0% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 39663
End of iteration 1, added 31704 new labels.
End of iteration 2, added 3628 new labels.
End of iteration 3, added 1667 new labels.
End of iteration 4, added 840 new labels.
End of iteration 5, added 18 new labels.
End of iteration 6, added 5 new labels.
End of iteration 7, added 348 new labels.
End of iteration 8, added 10 new labels.
End of iteration 9, added 45 new labels.
End of iteration 10, added 932 new labels.
Micro-averaged F1 score on test set: 0.824
----------

Supervised SGDClassifier on 33.416666666666664% of the training data:
Number of training samples: 19208
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.827
----------

SelfTrainingClassifier on 33.416666666666664% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 38274
End of iteration 1, added 23974 new labels.
End of iteration 2, added 5215 new labels.
End of iteration 3, added 2477 new labels.
End of iteration 4, added 180 new labels.
End of iteration 5, added 2827 new labels.
End of iteration 6, added 41 new labels.
End of iteration 7, added 268 new labels.
End of iteration 8, added 1062 new labels.
Micro-averaged F1 score on test set: 0.828
----------

Supervised SGDClassifier on 35.833333333333336% of the training data:
Number of training samples: 20597
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.828
----------

SelfTrainingClassifier on 35.833333333333336% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 36885
End of iteration 1, added 28501 new labels.
End of iteration 2, added 2172 new labels.
End of iteration 3, added 300 new labels.
End of iteration 4, added 1544 new labels.
End of iteration 5, added 496 new labels.
End of iteration 6, added 635 new labels.
End of iteration 7, added 10 new labels.
End of iteration 8, added 1 new labels.
End of iteration 9, added 1195 new labels.
End of iteration 10, added 16 new labels.
Micro-averaged F1 score on test set: 0.829
----------

Supervised SGDClassifier on 38.25% of the training data:
Number of training samples: 21986
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.823
----------

SelfTrainingClassifier on 38.25% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 35496
End of iteration 1, added 26715 new labels.
End of iteration 2, added 4108 new labels.
End of iteration 3, added 626 new labels.
End of iteration 4, added 66 new labels.
End of iteration 5, added 150 new labels.
End of iteration 6, added 81 new labels.
End of iteration 7, added 618 new labels.
End of iteration 8, added 10 new labels.
End of iteration 9, added 21 new labels.
End of iteration 10, added 659 new labels.
Micro-averaged F1 score on test set: 0.830
----------

Supervised SGDClassifier on 40.666666666666664% of the training data:
Number of training samples: 23376
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.820
----------

SelfTrainingClassifier on 40.666666666666664% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 34106
End of iteration 1, added 25465 new labels.
End of iteration 2, added 2648 new labels.
End of iteration 3, added 2102 new labels.
End of iteration 4, added 207 new labels.
End of iteration 5, added 1006 new labels.
End of iteration 6, added 11 new labels.
End of iteration 7, added 273 new labels.
End of iteration 8, added 166 new labels.
End of iteration 9, added 1 new labels.
End of iteration 10, added 1 new labels.
Micro-averaged F1 score on test set: 0.832
----------

Supervised SGDClassifier on 43.083333333333336% of the training data:
Number of training samples: 24765
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.822
----------

SelfTrainingClassifier on 43.083333333333336% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 32717
End of iteration 1, added 25463 new labels.
End of iteration 2, added 2722 new labels.
End of iteration 3, added 381 new labels.
End of iteration 4, added 1087 new labels.
End of iteration 5, added 3 new labels.
End of iteration 6, added 1027 new labels.
Micro-averaged F1 score on test set: 0.831
----------

Supervised SGDClassifier on 45.5% of the training data:
Number of training samples: 26154
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.827
----------

SelfTrainingClassifier on 45.5% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 31328
End of iteration 1, added 24924 new labels.
End of iteration 2, added 2006 new labels.
End of iteration 3, added 905 new labels.
End of iteration 4, added 52 new labels.
End of iteration 5, added 21 new labels.
End of iteration 6, added 4 new labels.
End of iteration 7, added 33 new labels.
End of iteration 8, added 247 new labels.
End of iteration 9, added 11 new labels.
End of iteration 10, added 79 new labels.
Micro-averaged F1 score on test set: 0.827
----------

Supervised SGDClassifier on 47.91666666666667% of the training data:
Number of training samples: 27543
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.829
----------

SelfTrainingClassifier on 47.91666666666667% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 29939
End of iteration 1, added 22413 new labels.
End of iteration 2, added 2460 new labels.
End of iteration 3, added 380 new labels.
End of iteration 4, added 1060 new labels.
End of iteration 5, added 552 new labels.
End of iteration 6, added 9 new labels.
End of iteration 7, added 766 new labels.
End of iteration 8, added 7 new labels.
End of iteration 9, added 1 new labels.
Micro-averaged F1 score on test set: 0.828
----------

Supervised SGDClassifier on 50.33333333333333% of the training data:
Number of training samples: 28932
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.829
----------

SelfTrainingClassifier on 50.33333333333333% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 28550
End of iteration 1, added 21735 new labels.
End of iteration 2, added 2067 new labels.
End of iteration 3, added 340 new labels.
End of iteration 4, added 283 new labels.
End of iteration 5, added 3 new labels.
Micro-averaged F1 score on test set: 0.830
----------

Supervised SGDClassifier on 52.75% of the training data:
Number of training samples: 30321
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.803
----------

SelfTrainingClassifier on 52.75% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 27161
End of iteration 1, added 20462 new labels.
End of iteration 2, added 824 new labels.
End of iteration 3, added 1542 new labels.
End of iteration 4, added 72 new labels.
End of iteration 5, added 557 new labels.
End of iteration 6, added 51 new labels.
End of iteration 7, added 27 new labels.
End of iteration 8, added 733 new labels.
End of iteration 9, added 51 new labels.
End of iteration 10, added 465 new labels.
Micro-averaged F1 score on test set: 0.831
----------

Supervised SGDClassifier on 55.166666666666664% of the training data:
Number of training samples: 31710
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.829
----------

SelfTrainingClassifier on 55.166666666666664% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 25772
End of iteration 1, added 19206 new labels.
End of iteration 2, added 3068 new labels.
End of iteration 3, added 131 new labels.
End of iteration 4, added 330 new labels.
End of iteration 5, added 57 new labels.
End of iteration 6, added 73 new labels.
End of iteration 7, added 520 new labels.
End of iteration 8, added 2 new labels.
End of iteration 9, added 13 new labels.
End of iteration 10, added 298 new labels.
Micro-averaged F1 score on test set: 0.829
----------

Supervised SGDClassifier on 57.58333333333333% of the training data:
Number of training samples: 33100
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.821
----------

SelfTrainingClassifier on 57.58333333333333% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 24382
End of iteration 1, added 16898 new labels.
End of iteration 2, added 594 new labels.
End of iteration 3, added 872 new labels.
End of iteration 4, added 1575 new labels.
End of iteration 5, added 480 new labels.
End of iteration 6, added 34 new labels.
End of iteration 7, added 59 new labels.
End of iteration 8, added 36 new labels.
End of iteration 9, added 2 new labels.
End of iteration 10, added 517 new labels.
Micro-averaged F1 score on test set: 0.830
----------

Supervised SGDClassifier on 60.0% of the training data:
Number of training samples: 34489
Unlabeled samples in training set: 0
Micro-averaged F1 score on test set: 0.830
----------

SelfTrainingClassifier on 60.0% of the training data (rest is unlabeled):
Number of training samples: 57482
Unlabeled samples in training set: 22993
End of iteration 1, added 19072 new labels.
End of iteration 2, added 180 new labels.
End of iteration 3, added 46 new labels.
End of iteration 4, added 51 new labels.
End of iteration 5, added 31 new labels.
End of iteration 6, added 54 new labels.
End of iteration 7, added 28 new labels.
End of iteration 8, added 7 new labels.
End of iteration 9, added 1400 new labels.
Micro-averaged F1 score on test set: 0.830
----------

--------------------------------
Model exists. Loading model for LogRegression
Made predictions in 0.0618 seconds.
ROC_AUC score for training set: 0.8046.
Made predictions in 0.0059 seconds.
ROC_AUC score for test set: 0.7908.

Classification Report: 
              precision    recall  f1-score   support

           0       0.36      0.74      0.48      2083
           1       0.93      0.70      0.80      9445

    accuracy                           0.71     11528
   macro avg       0.64      0.72      0.64     11528
weighted avg       0.82      0.71      0.74     11528

--------------------------------
--------------------------------
Model exists. Loading model for GaussianNB
Made predictions in 1.0263 seconds.
ROC_AUC score for training set: 0.7325.
Made predictions in 0.0539 seconds.
ROC_AUC score for test set: 0.7211.

Classification Report: 
              precision    recall  f1-score   support

           0       0.31      0.70      0.43      2083
           1       0.91      0.66      0.76      9445

    accuracy                           0.67     11528
   macro avg       0.61      0.68      0.60     11528
weighted avg       0.80      0.67      0.70     11528

--------------------------------
--------------------------------
Model exists. Loading model for DecisionTree
Made predictions in 0.0948 seconds.
ROC_AUC score for training set: 0.6619.
Made predictions in 0.0075 seconds.
ROC_AUC score for test set: 0.6563.

Classification Report: 
              precision    recall  f1-score   support

           0       0.69      0.02      0.04      2083
           1       0.82      1.00      0.90      9445

    accuracy                           0.82     11528
   macro avg       0.76      0.51      0.47     11528
weighted avg       0.80      0.82      0.75     11528

--------------------------------
--------------------------------
Model exists. Loading model for RandomForest
Made predictions in 0.6669 seconds.
ROC_AUC score for training set: 0.9004.
Made predictions in 0.2224 seconds.
ROC_AUC score for test set: 0.7693.

Classification Report: 
              precision    recall  f1-score   support

           0       0.96      0.01      0.02      2083
           1       0.82      1.00      0.90      9445

    accuracy                           0.82     11528
   macro avg       0.89      0.51      0.46     11528
weighted avg       0.85      0.82      0.74     11528

--------------------------------
--------------------------------
Model exists. Loading model for AdaBoost
Made predictions in 7.2293 seconds.
ROC_AUC score for training set: 0.7395.
Made predictions in 0.5071 seconds.
ROC_AUC score for test set: 0.7334.

Classification Report: 
              precision    recall  f1-score   support

           0       1.00      0.00      0.00      2083
           1       0.82      1.00      0.90      9445

    accuracy                           0.82     11528
   macro avg       0.91      0.50      0.45     11528
weighted avg       0.85      0.82      0.74     11528

--------------------------------
--------------------------------
Model exists. Loading model for SGD
Made predictions in 0.0174 seconds.
ROC_AUC score for training set: 0.8046.
Made predictions in 0.0046 seconds.
ROC_AUC score for test set: 0.7899.

Classification Report: 
              precision    recall  f1-score   support

           0       0.68      0.10      0.17      2083
           1       0.83      0.99      0.90      9445

    accuracy                           0.83     11528
   macro avg       0.75      0.54      0.54     11528
weighted avg       0.80      0.83      0.77     11528

--------------------------------

==================================

ENSEMBLE LEARNING RandomForest + AdaBoostClassifier

[Parallel(n_jobs=8)]: Done 784 tasks      | elapsed:    0.3s
[Parallel(n_jobs=8)]: Done 800 out of 800 | elapsed:    0.3s finished
Classification Report: 
              precision    recall  f1-score   support

           0       0.58      0.22      0.32      2083
           1       0.85      0.97      0.90      9445

    accuracy                           0.83     11528
   macro avg       0.72      0.59      0.61     11528
weighted avg       0.80      0.83      0.80     11528


# This code is borrowed from https://jmcauley.ucsd.edu/data/amazon/